{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f18e6f4b-3cbd-4a79-a537-6857d1a8cdae",
   "metadata": {},
   "source": [
    "Starting here with keyword extraction, geeksforgeeks hasn't let me down yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b131311-3518-4910-976e-c567352fa542",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/keyword-extraction-methods-in-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3abd7f5-bd77-4fbb-8e44-284c7df5ed30",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/creating-your-own-intent-classifier-b86e000a4926"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057df8e1-3887-4c6e-8a1d-7094dc6360e0",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/clean-text-machine-learning-python/#:~:text=The%20Natural%20Language%20Toolkit%2C%20or,learning%20and%20deep%20learning%20algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f36eb20-c4d5-4eb6-848f-30ba02a22166",
   "metadata": {},
   "source": [
    "https://www.nltk.org/book/ch03.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5ff0d-9372-41d9-87fb-5b7311645539",
   "metadata": {},
   "source": [
    "https://datasciencedojo.com/blog/rule-based-chatbot-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e946b2c-6401-477f-951a-9e4efc4e0d50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ahojd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ahojd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ahojd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['quick', 'brown', 'dog', 'lazy', 'jump', 'fox'], ['two', 'worth', 'bird', 'bush', 'hand'], ['louder', 'speak', 'word', 'action']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A bird in the hand is worth two in the bush\",\n",
    "    \"Actions speak louder than words\"\n",
    "]\n",
    "\n",
    "# Tokenize, lemmatize, and remove stopwords\n",
    "tokenized_data = []\n",
    "for sentence in data:\n",
    "    tokens = nltk.word_tokenize(sentence.lower())  # Tokenize and convert to lowercase\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize tokens\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]  # Remove stop words\n",
    "    tokenized_data.append(filtered_tokens)\n",
    "\n",
    "# Remove duplicate words\n",
    "for i in range(len(tokenized_data)):\n",
    "    tokenized_data[i] = list(set(tokenized_data[i]))\n",
    "\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87d39cbb-ce63-44dc-857c-b1b7eefc310e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: automatically identifies relevant keywords Score: 16.0\n",
      "Keyword: rapid automatic keyword extraction Score: 15.0\n",
      "Keyword: keyword extraction algorithm Score: 10.0\n",
      "Keyword: text document Score: 4.0\n",
      "Keyword: rake Score: 1.0\n",
      "Keyword: phrases Score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ahojd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ahojd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# installation see troubleshooting doc\n",
    "#!pip3 install rake-nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from rake_nltk import Rake\n",
    "\n",
    "# Create a Rake instance\n",
    "r = Rake()\n",
    "\n",
    "# Text from which keywords will be extracted\n",
    "text = \"RAKE (Rapid Automatic Keyword Extraction) is a keyword extraction algorithm that automatically identifies relevant keywords and phrases in a text document.\"\n",
    "\n",
    "# Extract keywords from the text\n",
    "r.extract_keywords_from_text(text)\n",
    "\n",
    "# Get the ranked keywords\n",
    "keywords = r.get_ranked_phrases_with_scores()\n",
    "\n",
    "# Print the extracted keywords and their scores\n",
    "for score, kw in keywords:\n",
    "    print(\"Keyword:\", kw, \"Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5a1265b-caf9-4bb3-b4d7-c7fb5cc429b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword: rapid automatic keyword extraction )., Score: 24.0\n",
      "Keyword: keyword extraction algorithm @@ Score: 17.0\n",
      "Keyword: automatically identifies relevant keywords Score: 16.0\n",
      "Keyword: text document Score: 4.0\n",
      "Keyword: rake Score: 1.0\n",
      "Keyword: phrases Score: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ahojd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ahojd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create a Rake instance\n",
    "r = Rake()\n",
    "\n",
    "# Text from which keywords will be extracted\n",
    "text = \"RAKE (Rapid Automatic Keyword Extraction)., is a keyword extraction algorithm @@that automatically identifies relevant keywords and phrases in a text document.\"\n",
    "\n",
    "# Extract keywords from the text\n",
    "r.extract_keywords_from_text(text)\n",
    "\n",
    "# Get the ranked keywords\n",
    "keywords = r.get_ranked_phrases_with_scores()\n",
    "\n",
    "# Print the extracted keywords and their scores\n",
    "for score, kw in keywords:\n",
    "    print(\"Keyword:\", kw, \"Score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973eea49-97ba-4892-aac2-0c5ad52fcdac",
   "metadata": {},
   "source": [
    "https://dataknowsall.com/blog/textcleaning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fc6185-3c43-4ba2-873b-b9eba92d229e",
   "metadata": {},
   "source": [
    "https://spacy.io/usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbae1b0-d11e-4d91-9253-7812f256b112",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# imports and load spacy\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7038e4-48e0-4ec5-a124-794682d182c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text from which keywords will be extracted\n",
    "text = \"RAKE (Rapid Automatic Keyword Extraction)., is a keyword extraction algorithm @@that automatically identifies relevant keywords and phrases in a text document.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7d1c98e-7536-48e5-95f8-03107275cdec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load \n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# define clean_string function:\n",
    "def clean_string(text, stem=\"None\"):\n",
    "\n",
    "    final_string = \"\"\n",
    "\n",
    "    # Make lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove line breaks\n",
    "    # Note: that this line can be augmented and used over\n",
    "    # to replace any characters with nothing or a space\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    useless_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    useless_words = useless_words + ['hi', 'im']\n",
    "\n",
    "    text_filtered = [word for word in text if not word in useless_words]\n",
    "\n",
    "    # Remove numbers\n",
    "    text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
    "\n",
    "    # Stem or Lemmatize\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer() \n",
    "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
    "    elif stem == 'Lem':\n",
    "        lem = WordNetLemmatizer()\n",
    "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
    "    elif stem == 'Spacy':\n",
    "        text_filtered = nlp(' '.join(text_filtered))\n",
    "        text_stemmed = [y.lemma_ for y in text_filtered]\n",
    "    else:\n",
    "        text_stemmed = text_filtered\n",
    "\n",
    "    final_string = ' '.join(text_stemmed)\n",
    "\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d57e81-eb3d-47e0-b044-21348ed7ce6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rake rapid automatic keyword extraction keyword extraction algorithm automatically identifies relevant keywords phrases text document'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_string(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d82935-49d4-4603-b065-e079ccf96cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an io.text.wrapper:\n",
    "data = open(\"./docs/TermsConditions_test.txt\", \"r\")\n",
    "print(data.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eebd9f-d25b-472c-b581-dd931e83f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is what I needed\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text_files = ['file1.txt', 'file2.txt']\n",
    "documents = [open(f) for f in text_files]\n",
    "tfidf = TfidfVectorizer().fit_transform(documents)\n",
    "# no need to normalize, since Vectorizer will return normalized tf-idf\n",
    "pairwise_similarity = tfidf * tfidf.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

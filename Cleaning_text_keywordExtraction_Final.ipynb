{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69cd9263-5f75-4a87-996f-5980ff769d1e",
   "metadata": {},
   "source": [
    "Removed the title page and table of contents pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b64988f-fb94-4186-a463-6e3b136aa412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read input data into a list\n",
    "data_file = './docs/TC_final.txt'\n",
    "with open(data_file, 'r') as input_data:\n",
    "    lst_data = input_data.read().splitlines()\n",
    "type(lst_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ac952-3c8f-4b81-8645-ed3a7cbd0176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lst_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003da16c-e532-42d0-8174-d15cdf2a7eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str_data1 = ' '.join([str(item) for item in lst_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fc500c-db7e-4782-8430-5d3cf0e0d111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1836bd79-6437-4e82-8089-315ca300e94c",
   "metadata": {},
   "source": [
    "My final cleanup version, before sending it through final cleanup package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda601a8-4341-4c8c-9f87-f78705520a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "str_data2 = ' '.join([str(item) for item in lst_data])\n",
    "str_data2 = str_data2.replace(' e ', '').replace('/', ' ').replace('(', '').replace(')', '').replace(',', '').replace(\"’\", '').replace('.', '').replace('  ', ' ')\\\n",
    ".replace(':', '').replace('\"', '').replace('”', '').replace(\"'\", '')\n",
    "\n",
    "str_data2 = str_data2.lower()\n",
    "\n",
    "print(str_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf7324-ba6b-406d-8709-892f4d13d5d2",
   "metadata": {},
   "source": [
    "Trying different cleaning packages - str_data1 = text before my cleanup, str_data2 = text with my cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2e752-f8d3-40e3-bb4a-624650fd82e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load \n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# define clean_string function:\n",
    "def clean_string(text, stem=\"None\"):\n",
    "\n",
    "    final_string = \"\"\n",
    "\n",
    "    # Make lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove line breaks\n",
    "    # Note: that this line can be augmented and used over\n",
    "    # to replace any characters with nothing or a space\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    useless_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    useless_words = useless_words + ['hi', 'im']\n",
    "\n",
    "    text_filtered = [word for word in text if not word in useless_words]\n",
    "\n",
    "    # Remove numbers\n",
    "    text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
    "\n",
    "    # Stem or Lemmatize\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer() \n",
    "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
    "    elif stem == 'Lem':\n",
    "        lem = WordNetLemmatizer()\n",
    "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
    "    elif stem == 'Spacy':\n",
    "        text_filtered = nlp(' '.join(text_filtered))\n",
    "        text_stemmed = [y.lemma_ for y in text_filtered]\n",
    "    else:\n",
    "        text_stemmed = text_filtered\n",
    "\n",
    "    final_string = ' '.join(text_stemmed)\n",
    "\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bd161-5280-433f-8d9d-c5f9a568209e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text from which keywords will be extracted\n",
    "text = \"RAKE (Rapid Automatic Keyword Extraction)., is a keyword extraction algorithm @@that automatically identifies relevant keywords and phrases in a text document.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e096661-e3da-46c7-8e99-feb30f76c9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_string(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d346f49-b8c0-4123-9de1-35dc6181b195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See how Spacy cleans str_data1\n",
    "clean_string(str_data1, stem=\"Spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acecfc7-28b4-4bea-ad99-5d5f786f6bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Between Lem and Spacy so far (none, Stem, Lem, Spacy) - Spacy removes a lot more useless words\n",
    "str_data_final = clean_string(str_data2, stem=\"Spacy\")\n",
    "str_data_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37bda4d-676b-42cf-91ba-9ab514e42565",
   "metadata": {},
   "source": [
    "By pre-cleaning the data, Spacy was able to work much better, the words are looking separated, clean, and most of the punctuation and use-less words removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12182985-810b-4123-ad38-2301c76a632d",
   "metadata": {},
   "source": [
    "I didn't play around with items on this website yet, but follows some interesting pre-processing steps:\n",
    "\n",
    "https://machinelearningmastery.com/clean-text-machine-learning-python/#:~:text=The%20Natural%20Language%20Toolkit%2C%20or,learning%20and%20deep%20learning%20algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a280a558-8403-410d-96ca-5fbe0319f249",
   "metadata": {},
   "source": [
    "A lot of good information on this site:\n",
    "\n",
    "https://www.nltk.org/book/ch03.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881525f8-a12b-46bf-86ae-a44aec8b7219",
   "metadata": {},
   "source": [
    "The version below is from this site:\n",
    "\n",
    "https://dataknowsall.com/blog/textcleaning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f265fb-c647-46b6-b87d-870e0abbdf3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A bird in the hand is worth two in the bush\",\n",
    "    \"Actions speak louder than words\"\n",
    "]\n",
    "\n",
    "# Tokenize, lemmatize, and remove stopwords\n",
    "tokenized_data = []\n",
    "for sentence in data:\n",
    "    tokens = nltk.word_tokenize(sentence.lower())  # Tokenize and convert to lowercase\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize tokens\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]  # Remove stop words\n",
    "    tokenized_data.append(filtered_tokens)\n",
    "\n",
    "# Remove duplicate words\n",
    "for i in range(len(tokenized_data)):\n",
    "    tokenized_data[i] = list(set(tokenized_data[i]))\n",
    "\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a9643-1c1e-4c6b-95a8-91024847b368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize, lemmatize, and remove stopwords\n",
    "\n",
    "data = [str_data_final]\n",
    "\n",
    "tokenized_data = []\n",
    "for sentence in data:\n",
    "    tokens = nltk.word_tokenize(sentence.lower())  # Tokenize and convert to lowercase\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize tokens\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]  # Remove stop words\n",
    "    tokenized_data.append(filtered_tokens)\n",
    "\n",
    "# Remove duplicate words\n",
    "for i in range(len(tokenized_data)):\n",
    "    tokenized_data[i] = list(set(tokenized_data[i]))\n",
    "\n",
    "print(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1878cea-28c0-4301-8bef-658dfb014dc7",
   "metadata": {},
   "source": [
    "I'm a little surprised the stopword feature doesn't work better"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f04b39c-f12c-4a26-b487-e81d1fd94c28",
   "metadata": {},
   "source": [
    "The Keyword Extraction below is from this site:\n",
    "\n",
    "https://www.geeksforgeeks.org/keyword-extraction-methods-in-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d143d93-7109-4d99-b9b0-c8d3407877c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#installation\n",
    "#!pip3 install spacy\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def get_hotwords(text):\n",
    "    result = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN'] \n",
    "    doc = nlp(text.lower()) \n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            result.append(token.text)\n",
    "    return result\n",
    "text = \"\"\"\n",
    "spaCy is an open-source natural language processing library for Python.\n",
    "\"\"\"\n",
    "output = set(get_hotwords(text))\n",
    "most_common_list = Counter(output).most_common(10)\n",
    "for item in most_common_list:\n",
    "  print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b818eec-1d02-48d0-92e1-fc5dc427a57e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = str_data_final\n",
    "output = set(get_hotwords(text))\n",
    "most_common_list = Counter(output).most_common(10)\n",
    "for item in most_common_list:\n",
    "  print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e33a3-d962-4194-a7c3-4aa14d170d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aap]",
   "language": "python",
   "name": "conda-env-aap-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
